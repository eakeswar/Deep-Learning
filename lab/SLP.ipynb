{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.6778987589627032\n",
      "Epoch 100, Loss: 0.6065460953116352\n",
      "Epoch 200, Loss: 0.41685083611232226\n",
      "Epoch 300, Loss: 0.3277902586435314\n",
      "Epoch 400, Loss: 0.2769445842628548\n",
      "Epoch 500, Loss: 0.23808678474759143\n",
      "Epoch 600, Loss: 0.2083962225320746\n",
      "Epoch 700, Loss: 0.1854733485380891\n",
      "Epoch 800, Loss: 0.16529961514058714\n",
      "Epoch 900, Loss: 0.14894189846420378\n",
      "Training Accuracy: 98.33333333333333%\n",
      "Test Accuracy: 6.666666666666667%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "# Map string labels to integers\n",
    "def label_to_int(labels):\n",
    "    label_map = {\"iris-setosa\": 0, \"iris-versicolor\": 1, \"iris-virginica\": 2}\n",
    "    # Strip any leading/trailing spaces and convert to lowercase for consistency\n",
    "    return np.array([label_map[label.strip().lower()] for label in labels])\n",
    "\n",
    "# Load the Iris dataset from a CSV file\n",
    "data = pd.read_csv('iris.csv')\n",
    "\n",
    "# Assuming the last column is the label and the rest are features\n",
    "X = data.iloc[:, :-1].values  # All columns except the last one (features)\n",
    "y = data.iloc[:, -1].values   # Only the last column (label)\n",
    "\n",
    "# Convert string labels to integers\n",
    "y = label_to_int(y)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_encoded = one_hot_encode(y, 3)\n",
    "\n",
    "# Normalize the features (Standardization)\n",
    "def normalize(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    return (X - mean) / std\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "    m = X.shape[0]\n",
    "    split_idx = int(m * (1 - test_size))\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)\n",
    "\n",
    "# Neural Network Hyperparameters\n",
    "input_size = X_train.shape[1]  # Number of features (4 for Iris)\n",
    "hidden_size = 4  # Number of neurons in the hidden layer\n",
    "output_size = y_train.shape[1]  # Number of classes (3 for Iris)\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights from input to hidden layer\n",
    "b1 = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # Weights from hidden to output layer\n",
    "b2 = np.zeros((1, output_size))  # Bias for output layer\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Loss function (Cross-entropy)\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "# Forward pass\n",
    "def forward(X):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    return A1, A2\n",
    "\n",
    "# Backward pass (Gradient descent)\n",
    "def backward(X, y, A1, A2, learning_rate=0.01):\n",
    "    global W1, b1, W2, b2\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Compute the gradients\n",
    "    dZ2 = A2 - y  # Derivative of loss with respect to Z2\n",
    "    dW2 = np.dot(A1.T, dZ2) / m\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
    "    \n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * (A1 > 0)  # Derivative of ReLU\n",
    "    dW1 = np.dot(X.T, dZ1) / m\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Train the model\n",
    "def train(X_train, y_train, epochs=1000, learning_rate=0.01):\n",
    "    for epoch in range(epochs):\n",
    "        A1, A2 = forward(X_train)\n",
    "        loss = compute_loss(y_train, A2)\n",
    "        global W1, b1, W2, b2\n",
    "        W1, b1, W2, b2 = backward(X_train, y_train, A1, A2, learning_rate)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Prediction function\n",
    "def predict(X):\n",
    "    _, A2 = forward(X)\n",
    "    return np.argmax(A2, axis=1)\n",
    "\n",
    "# Train the model\n",
    "W1, b1, W2, b2 = train(X_train, y_train, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_train = predict(X_train)\n",
    "y_pred_test = predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = np.mean(y_pred_train == np.argmax(y_train, axis=1)) * 100\n",
    "test_accuracy = np.mean(y_pred_test == np.argmax(y_test, axis=1)) * 100\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy}%\")\n",
    "print(f\"Test Accuracy: {test_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
